{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Promemoria: questo üìò `.NET Interactive` deve essere eseguito da VS Code con [questi prerequisiti](../PREREQS.md).\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### How to use this notebook: \n",
    "\n",
    "* Just read the text and scroll along until you run into code blocks.\n",
    "* Code blocks have computer code inside them ‚Äî hover over the block and you can run the code.\n",
    "* Run the code by hitting the ‚ñ∂Ô∏è \"play\" button to the left. If the code runs you'll see a ‚úîÔ∏è. If not, you'll get a ‚ùå.\n",
    "* The output and status of the code block will appear just below itself ‚Äî you need to scroll down further to see it.\n",
    "* Sometimes a code block will ask you for input in a hard-to-notice dialog box üëÜ at the top of your notebook window.  -->\n",
    "\n",
    "<!-- --- -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe IV. üçù Memories\n",
    "<!-- ## üßë‚Äçüç≥ Cook well beyond the model's memory limits -->\n",
    "\n",
    "\n",
    "La lunghezza di una richiesta dipende dal modello di LLM in uso.\\\n",
    "I modelli pi√π recenti possono accettare richieste pi√π lunghe, mentre quelli pi√π vecchi possono accettare solo richieste pi√π brevi.\\\n",
    "Di conseguenza, c'√® un limite alla quantit√† di contesto che si pu√≤ fornire in un determinato prompt.\n",
    "\n",
    "| Model | Maximum Tokens** |\n",
    "|---|---|\n",
    "| ada | 2049 |\n",
    "| babbage | 2049 |\n",
    "| curie-001 | 2049 |\n",
    "| davinci-003 | 4097 |\n",
    "| GPT-4 | 8192 |\n",
    "\n",
    "** _1 token corrisponde a circa 3 caratteri; 1 pagina del libro corrisponde a circa 500 token._\n",
    "\n",
    "Un metodo che sta diventando sempre pi√π popolare √® quello di utilizzare il cosiddetto \"**embedding**\", che consiste nel rappresentare del testo come vettori numerici di grandi dimensioni.\n",
    "\n",
    "Quando si utilizzano i modelli OpenAI o Azure OpenAI Service, il modello `ada` √® una scelta economica e sufficiente per la maggior parte dei casi d'uso.\\\n",
    "Cominciamo a imparare generando alcuni embeddings e vediamo come funzionano in pratica.\n",
    "\n",
    "## Step 1. Istanziare un üî• kernel sia per il completamento che per la generazione di incorporazioni.\n",
    "\n",
    "Si noti che il codice sottostante include alcune nuove righe che si riferiscono all'uso del modello `text-embedding-ada-002` da usare per generare il vettore di numeri per un pezzo di testo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.SemanticKernel, 0.9.61.1-preview</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.SemanticKernel, 0.9.61.1-preview\"\n",
    "\n",
    "#!import ../config/Settings.cs\n",
    "\n",
    "using Microsoft.SemanticKernel;\n",
    "using Microsoft.SemanticKernel.KernelExtensions;\n",
    "using System.IO;\n",
    "using Microsoft.SemanticKernel.Configuration;\n",
    "using Microsoft.SemanticKernel.SemanticFunctions;\n",
    "using Microsoft.SemanticKernel.CoreSkills;\n",
    "using Microsoft.SemanticKernel.Memory;\n",
    "\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId) = Settings.LoadFromFile();\n",
    "\n",
    "var kernel =  Microsoft.SemanticKernel.Kernel.Builder\n",
    ".Configure(c =>\n",
    "{\n",
    "    if (useAzureOpenAI) {\n",
    "        c.AddAzureOpenAITextCompletion(\"davinci\", model, azureEndpoint, apiKey);\n",
    "        c.AddAzureOpenAIEmbeddingGeneration(\"ada\", \"text-embedding-ada-002\", azureEndpoint, apiKey);\n",
    "    } else {\n",
    "        c.AddOpenAITextCompletion(\"davinci\", model, apiKey, orgId);\n",
    "        c.AddOpenAIEmbeddingGeneration(\"ada\", \"text-embedding-ada-002\", apiKey, orgId);\n",
    "    }\n",
    "})\n",
    ".WithMemoryStorage(new VolatileMemoryStore())\n",
    ".Build();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Aggiungere üçù memories per permettere al üî• kernel di cucinare piatti pi√π ricchi\n",
    "\n",
    "Immaginate una raccolta di fatti raccolti su di voi su Internet come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four GIGANTIC vectors were generated just now from those 4 pieces of text above.\r\n"
     ]
    }
   ],
   "source": [
    "const string memoryCollectionName = \"Facts About Me\";\n",
    "\n",
    "await kernel.Memory.SaveInformationAsync(memoryCollectionName, id: \"LinkedIn Bio\", \n",
    "    text: \"I currently work in the hotel industry at the front desk. I won the best team player award.\");\n",
    "\n",
    "await kernel.Memory.SaveInformationAsync(memoryCollectionName, id: \"LinkedIn History\", \n",
    "    text: \"I have worked as a tourist operator for 8 years. I have also worked as a banking associate for 3 years.\");\n",
    "\n",
    "await kernel.Memory.SaveInformationAsync(memoryCollectionName, id: \"Recent Facebook Post\", \n",
    "    text: \"My new dog Trixie is the cutest thing you've ever seen. She's just 2 years old.\");\n",
    "    \n",
    "await kernel.Memory.SaveInformationAsync(memoryCollectionName, id: \"Old Facebook Post\", \n",
    "    text: \"Can you believe the size of the trees in Yellowstone? They're huge! I'm so committed to forestry concerns.\");\n",
    "\n",
    "Console.WriteLine(\"Four GIGANTIC vectors were generated just now from those 4 pieces of text above.\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚úÖ √à necessario avere accesso al modello `text-embedding-ada-002` per eseguire correttamente quanto sopra. Si noti che il passo 1 per questa unit√† √® diverso da tutti gli altri notebook perch√© ha questo requisito in pi√π per funzionare.\n",
    "\n",
    "Immaginate poi di voler porre al vostro LLM una domanda su di voi. Cosa farebbe?\\\n",
    "Dato che non sa nulla di voi, si **inventer√†** semplicemente qualcosa su di voi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You're an experienced professional with a passion for excellence.\r\n"
     ]
    }
   ],
   "source": [
    "// Create the semantic function\n",
    "var myFunction = kernel.CreateSemanticFunction(@\"\n",
    "Tell me about me and {{$input}} in less than 70 characters.\n",
    "\", maxTokens: 100, temperature: 0.8, topP: 1);\n",
    "\n",
    "// Invoke the semantic function passing \"my work history\" as the input\n",
    "var result = await myFunction.InvokeAsync(\"my work history\");\n",
    "\n",
    "Console.WriteLine(result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ad esempio, la funzione semantica di cui sopra potrebbe dire:\n",
    "\n",
    "`You are a creative problem solver with a varied work history.`\n",
    "\n",
    "Questo potrebbe valere per chiunque, ovviamente :).\n",
    "\n",
    "Invece di sperare che l'LLM fornisca la risposta pi√π corretta, possiamo usare le üçù **memories** per creare un completamento pi√π accurato.\\\n",
    "Lo facciamo trovando le memories **pi√π simili** che abbiamo salvato, cercando tra le memorie memorizzate, assegnando il numero massimo di risultati che vogliamo ottenere con `limit` e impostando una soglia di rilevanza per la ricerca con `minRelevanceScore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "  >> LinkedIn History\n",
      "  Text: I have worked as a tourist operator for 8 years. I have also worked as a banking associate for 3 years.  Relevance: 0,8252466106558247\n",
      "\n",
      "Result 2:\n",
      "  >> LinkedIn Bio\n",
      "  Text: I currently work in the hotel industry at the front desk. I won the best team player award.  Relevance: 0,8025544060686295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string ask = \"Tell me about me and my work history.\";\n",
    "var relatedMemory = \"I know nothing.\";\n",
    "var counter = 0;\n",
    "\n",
    "var memories = kernel.Memory.SearchAsync(memoryCollectionName, ask, limit: 5, minRelevanceScore: 0.77);\n",
    "\n",
    "await foreach (MemoryQueryResult memory in memories)\n",
    "{\n",
    "    // The first result is the most relevant\n",
    "    if (counter == 0) { relatedMemory = memory.Text; }\n",
    "    Console.WriteLine($\"Result {++counter}:\\n  >> {memory.Id}\\n  Text: {memory.Text}  Relevance: {memory.Relevance}\\n\");\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo porre la stessa domanda, ma con il contesto pi√π rilevante che abbiamo memorizzato in `relatedMemory` per ottenere una risposta pi√π accurata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have a diverse work history with a variety of skills and experiences.\r\n"
     ]
    }
   ],
   "source": [
    "var myFunction = kernel.CreateSemanticFunction(@\"\n",
    "{{$input}}\n",
    "Tell me about me and my work history in less than 70 characters.\n",
    "\", maxTokens: 100, temperature: 0.1, topP: .1);\n",
    "\n",
    "var result = await myFunction.InvokeAsync(relatedMemory);\n",
    "\n",
    "Console.WriteLine(result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preparatevi per il momento **WOW**.\n",
    "\n",
    "<!-- ### Manipulating üçù memories is how the token window limitation is addressed. -->\n",
    "\n",
    "<!-- Recall the table showing the maximum tokens that can be used per model:\n",
    "\n",
    "| Model | Maximum Tokens** |\n",
    "|---|---|\n",
    "| ada | 2049 |\n",
    "| babbage | 2049 |\n",
    "| curie-001 | 2049 |\n",
    "| davinci-003 | 4097 |\n",
    "| GPT-4 | 8192 |\n",
    "\n",
    "** _1 token is approximately 3 characters; 1 page of book is roughly 500 tokens_ -->\n",
    "\n",
    "<!-- Given this same basic technique of gathering the most similar memories that are appropriate to a prompt, it's possible to have many more memories stored and available on-hand to compare with a given prompt. And it's not necessary to include just the top hit, but also more hits that are just as similar to the \"most relevant\" memory available. \n",
    "\n",
    "This is how an entire book can be used by Semantic Kernel as a memory source to feed into a prompt by only selecting the relevant chunks of text ‚Äî i.e. that which relates to the prompt. To do so you would:\n",
    "\n",
    "1. Generate embeddings for each of the paragraphs in the book.\n",
    "2. For a given prompt, find the most similar paragraphs within the book.\n",
    "3. Staying within the limitation of the token size window, gather all the related paragraphs.\n",
    "4. You now have a prompt with a great deal of relevant ü•ë context to send to the model.\n",
    "5. Reap the benefits of an \"informed\" LLM AI weighing in on a particular subject for you.\n",
    "\n",
    "Let's review this in practice. Say I have a 500-page book. \n",
    "\n",
    "1. I take each page and generate the embedding with `Memory.SaveInformationAsync`\n",
    "2. I then take my prompt, `the best scenes are ones with flowers in it and deserve to be summarized` and use `Memory.SearchAsync` to locate the pages with flower scenes in them.\n",
    "3. Let's say there are three pages that are relevant. Those three pages will be used to compose a new prompt that's simply the three pages appended to each other along with the original prompt. If instead you need to include ten pages, and exceed the token window, then summarize each of the ten pages separately into ten shorter passages. Do this until you meet the token window requirements.\n",
    "4. You have the prompt to give to the model you've chosen. It has pulled the relevant information out of the 500-page book, and will do its best to summarize what you care about the most.\n",
    "5. Ta-da! You'll get what you've asked for. -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra\n",
    "\n",
    "Per illustrare questo punto, possiamo prendere il famoso discorso di Gettysburg di Abraham Lincoln e usarlo per generare un nuovo discorso:\n",
    "\n",
    "- üß© Dividiamo l'intero discorso in \"chunk\";\n",
    "- üî¢ Utilizziamo `ada` per fare l'embedding attraverso `kernel.Memory.SaveInformationAsync()`;\n",
    "- üîç Utilizziamo l'API Azure Cognitive Search per cercare il chunk pi√π simile alla nostra richiesta.\n",
    "\n",
    "Questo ci da un'idea di come pu√≤ essere elaborato un file di grandi dimensioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: Four score and seven years ago our fathers brought forth upon this continent a new nation, conceived in liberty, and dedicated to the proposition\n",
      "Chunk 1: that all men are created equal. (Applause.) Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived\n",
      "Chunk 2: and so dedicated, can long endure. We are met on a great battle field of that war; we are met to dedicate a portion of it as the final resting\n",
      "Chunk 3: place of those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this, but in a larger\n",
      "Chunk 4: sense, we cannot dedicate, we cannot consecrate, we cannot hallow this ground.\n",
      "The brave men, living and dead, who struggled here have consecrated\n",
      "Chunk 5: it far above our poor power to add or to detract. (Applause.) The world will little note, nor long remember, what we say here; but it can never\n",
      "Chunk 6: forget what they did here. (Applause.) It is for us, the living, rather to be dedicated here to the unfinished work that they have thus far\n",
      "Chunk 7: so nobly carried on. (Applause.) It is rather for us here to be dedicated to the great task remaining before us; that from these honored dead\n",
      "Chunk 8: we take increased devotion to that cause for which they gave the last full measure of devotion; that we here highly resolve that the dead shall\n",
      "Chunk 9: not have died in vain. (Applause.) That the nation shall, under God, have a new birth of freedom, and that the government of the people, by\n",
      "Chunk 10: the people and for the people, shall not perish from the earth. (Long applause.)\n"
     ]
    }
   ],
   "source": [
    "using System;\n",
    "using System.IO;\n",
    "using System.Text;\n",
    "\n",
    "public static List<string> ChunkTextFile(string filePath, int recommendedLength)\n",
    "{\n",
    "    List<string> chunks = new List<string>();\n",
    "\n",
    "    // Read in the text file\n",
    "    string text = File.ReadAllText(filePath);\n",
    "\n",
    "    // Break the text into chunks of the recommended length\n",
    "    int startIndex = 0;\n",
    "    while (startIndex < text.Length)\n",
    "    {\n",
    "        int endIndex = startIndex + recommendedLength;\n",
    "        if (endIndex > text.Length)\n",
    "        {\n",
    "            endIndex = text.Length;\n",
    "        }\n",
    "\n",
    "        // Look for a natural breakage point like a paragraph or just before a new heading\n",
    "        while (endIndex < text.Length && !char.IsWhiteSpace(text[endIndex]))\n",
    "        {\n",
    "            endIndex++;\n",
    "        }\n",
    "\n",
    "        // Get the chunk of text\n",
    "        string chunk = text.Substring(startIndex, endIndex - startIndex);\n",
    "\n",
    "        // Strip the whitespace at the start and end of the string\n",
    "        chunk = chunk.Trim();\n",
    "\n",
    "        // Add the chunk to the list\n",
    "        chunks.Add(chunk);\n",
    "\n",
    "        // Move to the next chunk\n",
    "        startIndex = endIndex;\n",
    "    }\n",
    "\n",
    "    return chunks;\n",
    "}\n",
    "\n",
    "// Get the list of chunks from the file\n",
    "List<string> chunks = ChunkTextFile(\"./lincoln.txt\", 140);\n",
    "\n",
    "const string lincolnMemoryCollectionName = \"Abe's Words\";\n",
    "\n",
    "// Add the chunks to memory\n",
    "int counter = 0;\n",
    "foreach (string chunk in chunks)\n",
    "{\n",
    "    Console.WriteLine($\"Chunk {counter}: {chunk}\");\n",
    "\n",
    "    await kernel.Memory.SaveInformationAsync(lincolnMemoryCollectionName, id: $\"Chunk {counter++}\", \n",
    "        text: chunk);\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo ora interrogare questi chunk per trovare quelli pi√π simili che corrispondono a una semplice domanda: `\"What should the people do?\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "  >> Chunk 10\n",
      "  Text: the people and for the people, shall not perish from the earth. (Long applause.)  Relevance: 0,8015134934072992\n",
      "\n",
      "Result 2:\n",
      "  >> Chunk 6\n",
      "  Text: forget what they did here. (Applause.) It is for us, the living, rather to be dedicated here to the unfinished work that they have thus far  Relevance: 0,7704788293903422\n",
      "\n",
      "Memory to feed back into the prompt will be:\n",
      "  >> the people and for the people, shall not perish from the earth. (Long applause.) forget what they did here. (Applause.) It is for us, the living, rather to be dedicated here to the unfinished work that they have thus far \n",
      "\n",
      "Generated response ... 'according to Lincoln':\n",
      "\n",
      "The people should continue the unfinished work of those who have gone before them, and strive to create a better future for all.\n"
     ]
    }
   ],
   "source": [
    "var aCounter = 0;\n",
    "var myPrompt = \"What should the people do?\";\n",
    "var myMemory = \"\";\n",
    "var memories = kernel.Memory.SearchAsync(lincolnMemoryCollectionName, myPrompt, limit: 5, minRelevanceScore: 0.77);\n",
    "\n",
    "await foreach (MemoryQueryResult memory in memories) {\n",
    "    Console.WriteLine($\"Result {++aCounter}:\\n  >> {memory.Id}\\n  Text: {memory.Text}  Relevance: {memory.Relevance}\\n\");\n",
    "    myMemory += memory.Text + \" \";\n",
    "}\n",
    "\n",
    "Console.WriteLine(\"Memory to feed back into the prompt will be:\\n  >> \" + myMemory+ \"\\n\");\n",
    "var myLincolnFunction = kernel.CreateSemanticFunction(@\"\n",
    "Lincoln said:\n",
    "---\n",
    "{{$input}}\n",
    "---\n",
    "So what should the people do?\n",
    "\", maxTokens: 100, temperature: 0.1, topP: .1);\n",
    "\n",
    "var lincolnResult = await myLincolnFunction.InvokeAsync(myMemory);\n",
    "\n",
    "Console.WriteLine(\"Generated response ... 'according to Lincoln':\\n\" + lincolnResult);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un esempio su larga scala, √® l'applicazione di esempio disponibile su GitHub Q&A at [https://aka.ms/sk/repo](https://aka.ms/sk/repo).\\\n",
    "Prende un intero repo di codice, lo converte in embeddings e permette di \"chattare\" con il repo stesso. Tenete presente che sarebbe generalmente impossibile inserire l'intero repo nella finestra di un'intelligenza artificiale LLM, ed √® qui che entra in gioco l'uso delle üçù memories ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è≠Ô∏è I prossimi passi\n",
    "\n",
    "<!-- Run through more advanced examples in the notebooks that are available in our GitHub repo at [https://aka.ms/sk/repo](https://aka.ms/sk/repo). -->\n",
    "\n",
    "[Vediamo i üßÑ Connectors!](../e5-connectors/notebook.ipynb)\n",
    "\n",
    "<!-- Or stay a longer while and add more facts about yourself in the `MemoryCollection`. -->"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
